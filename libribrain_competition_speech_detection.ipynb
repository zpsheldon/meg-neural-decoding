{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zpsheldon/meg-neural-decoding/blob/main/libribrain_competition_speech_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPcAt5RyxJE2"
      },
      "source": [
        "# 🍍 LibriBrain Competition: Getting started (Speech Detection)\n",
        "Welcome! This Notebook is the entrypoint into the [LibriBrain](https://libribrain.com/) competition hosted by the [PNPL](pnpl.robots.ox.ac.uk/) at NeurIPS 2025.\n",
        "\n",
        "It will walk you through\n",
        "1. setting up all necessary dependencies,\n",
        "2. downloading training data, and\n",
        "3. training a scaled down version of the simplest track of the competition\n",
        "\n",
        "It is fully functional in the Colab Free Tier, though training will of course be faster with more GPU horsepower. With default settings on a `T4` instance, the main training run should take no more than 45 minutes.\n",
        "\n",
        "In case of any questions or problems, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord).\n",
        "\n",
        "⚠️ **Notes**:\n",
        "- If you want to speed up model training, **make sure you are on a GPU runtime by clicking Runtime -> Change runtime type**. TPU acceleration is currently not supported.\n",
        "- We have only comprehensively validated the notebook to work on Colab and Unix. Your experience in other environments (e.g., Windows) may vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMX3YFxmyu2F"
      },
      "source": [
        "## Setting up dependencies\n",
        "Run the code below *as is*. It will download all required dependencies, including our own [PNPL](https://pypi.org/project/pnpl/) package. On Windows, you might have to restart your Kernel after the installation has finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FGSilADiE1g4"
      },
      "outputs": [],
      "source": [
        "# Install additional depdendencies\n",
        "%pip install -q lightning torchmetrics scikit-learn plotly ipywidgets pnpl\n",
        "\n",
        "# Set up base path for dataset and related files (base_path is assumed to be set in the cells below!)\n",
        "base_path = \"./libribrain\"\n",
        "try:\n",
        "    import google.colab  # This module is only available in Colab.\n",
        "    in_colab = True\n",
        "    base_path = \"/content\"  # This is the folder displayed in the Colab sidebar\n",
        "except ImportError:\n",
        "    in_colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzYOCXRSyz4h"
      },
      "source": [
        "## Understanding the dataset\n",
        "What makes the LibriBrain interesting is that it is _deep_ and not just _big_. It contains data from a single person listening to stories from the [canon of Sherlock Holmes](https://en.wikipedia.org/wiki/Canon_of_Sherlock_Holmes) by Sir Arthur Doyle for around **50 hours**, giving us plenty of data to work with.\n",
        "\n",
        "Let's first set up the smallest possible dataset, using only a single session of a single task. Doing this will automatically download the necessary data. We're using the following parameters:\n",
        "- `data_path`: Local path where the MEG data and event files should be stored\n",
        "- `include_run_keys`: \"BIDS-like\" specification of which data you wish to load. Expects a list of tuples like *(subject, session, task, run)*\n",
        "- `tmin` & `tmax`: How long we want one input into the model to be in seconds. For 0.8s at our sampling rate of 250Hz, this amounts to 250*0.8=200 samples.\n",
        "\n",
        "Note: The `LibriBrainSpeech` dataset will provide simple speech/silence labels. For more complex tasks like phoneme classification, we also provide the `LibriBrainPhoneme` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WPI5FZIPcnM"
      },
      "outputs": [],
      "source": [
        "from pnpl.datasets import LibriBrainSpeech\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# We're only using data from\n",
        "# - subject 0 (all LibriBrain data is from the same subject!)\n",
        "# - session 1 (this represents chapter of the book)\n",
        "# - task \"Sherlock1\" (meaning the first book of the cannon)\n",
        "# - run 1 (collected in the first run)\n",
        "# for now.\n",
        "# We'll load more later!\n",
        "example_data = LibriBrainSpeech(\n",
        "  data_path=f\"{base_path}/data/\",\n",
        "  include_run_keys = [(\"0\",\"1\",\"Sherlock1\",\"1\")],\n",
        "  tmin=0.0,\n",
        "  tmax=0.8,\n",
        "  preload_files = True\n",
        ")\n",
        "\n",
        "# Conditionally set num_workers to avoid multiprocessing issues (try increasing if performance is problematic)\n",
        "num_workers = 2 if in_colab else 0\n",
        "\n",
        "example_loader = DataLoader(example_data, batch_size=32, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "print(\"Number of samples:\", len(example_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uudwXbvh_Pvy"
      },
      "source": [
        "### Exploring the data\n",
        "Before we start _doing_ anything, let's explore the dataset. If you check the path set as `data_path` above (`/content/data` in Colab by default), you will see a long list of two types of files\n",
        "- **MEG data**: Larger files ending in [`.h5`](https://www.hdfgroup.org/solutions/hdf5/), these contain the MEG data for each session. The data has been preprocessed.\n",
        "- **Event files**: Ending in [`.tsv`](https://en.wikipedia.org/wiki/Tab-separated_values), these contain information about which words/phonemes occur with the corresponding timestamps. For our purposes, they function as labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_8RLKsw-xk4"
      },
      "source": [
        "#### HDF5 files\n",
        "Let's start with the HDF5 files.\n",
        "First, let's take a look at the actual file name of what we just downloaded:\n",
        "\n",
        "`sub-0_ses-1_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5`\n",
        "\n",
        "This can seem daunting at first, so let's break it down:\n",
        "- `sub-0`: The data comes from Subject 1\n",
        "- `ses-1`: and has been collected during session 1.\n",
        "- `task-Sherlock1`: It contains data related to the Sherlock1 task,\n",
        "- `run-1` specifically the first run of it\n",
        "\n",
        "The remaining part indicates the preprocessing we have performed:\n",
        "- `bads`: Removal of [bad channels](https://mne.tools/stable/auto_tutorials/preprocessing/15_handling_bad_channels.html)\n",
        "- `headpos`: Adjusted the signal to [compensate for head movement](https://mne.tools/stable/auto_tutorials/preprocessing/60_maxwell_filtering_sss.html)\n",
        "- `sss`: [Signal-space separation](https://mne.tools/stable/auto_tutorials/preprocessing/60_maxwell_filtering_sss.html), used to isolate signals from inside the head from those coming from outside it\n",
        "- `notch`: [Notch filtering](https://mne.tools/stable/generated/mne.filter.notch_filter.html), used to eliminate specific frequency noise (e.g., from the electrical grid)\n",
        "- `bp`: Bandpass [filtering](https://mne.tools/1.8/auto_tutorials/preprocessing/30_filtering_resampling.html), used to isolate the frequency range of interest\n",
        "- `ds`: [Downsampling](https://mne.tools/stable/generated/mne.filter.resample.html), reducing the data's sampling rate for easier handling\n",
        "- `meg` The modality of the brain data. The LibriBrain data was collected using [magnetoencephalography](https://en.wikipedia.org/wiki/Magnetoencephalography).\n",
        "\n",
        "The data structure in the file is relatively straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLZq9cJ-QqpB"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This is the file we'll look at\n",
        "hdf5_file_path = f\"{base_path}/data/Sherlock1/derivatives/serialised/sub-0_ses-1_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5\"\n",
        "\n",
        "with h5py.File(hdf5_file_path, 'r') as f:\n",
        "    print(\"=== HDF5 File Structure ===\")\n",
        "    def print_structure(name, obj):\n",
        "        obj_type = \"Group\" if isinstance(obj, h5py.Group) else \"Dataset\"\n",
        "        print(f\"{name} ({obj_type})\")\n",
        "    f.visititems(print_structure)\n",
        "    for key in f.keys():\n",
        "        obj = f[key]\n",
        "        print(f\"\\n--- Details for '{key}' ---\")\n",
        "        print(\"Type:\", \"Group\" if isinstance(obj, h5py.Group) else \"Dataset\")\n",
        "        if hasattr(obj, 'shape'):\n",
        "            print(\"Shape:\", obj.shape)\n",
        "            print(\"Data type:\", obj.dtype)\n",
        "    data = f[\"data\"][:]   # 'data' holds the actual recorded MEG sensor values\n",
        "    times = f[\"times\"][:] # 'times' holds the corresponding time stamps\n",
        "\n",
        "print(\"\\n--- Statistics for 'data' dataset ---\")\n",
        "print(\"Minimum value:\", np.min(data))\n",
        "print(\"Maximum value:\", np.max(data))\n",
        "print(\"Mean value:\", np.mean(data))\n",
        "print(\"Standard deviation:\", np.std(data))\n",
        "\n",
        "print(\"\\nAs you can see, timestamp is 0.004 seconds from the previous:\")\n",
        "print(times[:10])\n",
        "print(\"This equals a polling rate of 250 Hz.\")\n",
        "\n",
        "print(\"\\nAnd these are the first first 10 time samples from the first 5 channels of the data:\")\n",
        "print(data[:5, :10])\n",
        "\n",
        "\n",
        "print(times[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8lEzO1u9-F4"
      },
      "source": [
        "Next, let's get a visual overview over the locations of the sensors and their corresponding values.\n",
        "Using the settings at the bottom, we can explore the different timestamps and inspect the sensors activity over time.\n",
        "We can choose to use the sensors's absolute values (using a logarithmic scale to deal with some outliers) or switch to a normalized version in which the sensor's values are normalized for each sensor.\n",
        "\n",
        "⚠️ **Note**: In reality, we have sets of three sensors sharing the same location (306 sensors at 102 locations), each capturing a different axis of the magnetic field. For the sake of the visualization below, the second and third sensor at the same location are offset slightly (as they would be invisible otherwise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aB6CExQxJsc"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import VBox, HBox\n",
        "from IPython.display import display\n",
        "import json\n",
        "import os, requests\n",
        "\n",
        "\n",
        "# Enable custom widgets (in Colab)\n",
        "if in_colab:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "\n",
        "\n",
        "# Download sensor locations JSON\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "p = os.path.join(base_path, \"sensor_xyz.json\")\n",
        "if not os.path.exists(p):\n",
        "    with open(p, \"wb\") as f: f.write(requests.get(\"https://neural-processing-lab.github.io/2025-libribrain-competition/sensor_xyz.json\").content)\n",
        "    print(\"Downloaded sensor_xyz.json\")\n",
        "else:\n",
        "    print(\"File exists, skipping download.\")\n",
        "\n",
        "\n",
        "# Load sensor positions from exported JSON file\n",
        "with open(p, \"r\") as fp:\n",
        "    sensor_xyz = np.array(json.load(fp))\n",
        "n_channels = sensor_xyz.shape[0]\n",
        "\n",
        "# Sampling frequency\n",
        "sfreq = 250\n",
        "\n",
        "# Load the MEG data and timestamps from the new HDF5 file\n",
        "hdf5_file_path = f\"{base_path}/data/Sherlock1/derivatives/serialised/sub-0_ses-1_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5\"\n",
        "with h5py.File(hdf5_file_path, \"r\") as f:\n",
        "    raw_data = f[\"data\"][:]\n",
        "    times = f[\"times\"][:]\n",
        "\n",
        "# Determine total duration from the timestamps\n",
        "total_time_sec = times[-1]\n",
        "n_timepoints = raw_data.shape[1]\n",
        "\n",
        "print(f\"Data shape: {raw_data.shape}\")\n",
        "print(f\"Total duration: {total_time_sec:.1f} seconds\")\n",
        "\n",
        "# Precompute per-sensor normalization (avoid division by zero)\n",
        "sensor_mins = raw_data.min(axis=1, keepdims=True)\n",
        "sensor_maxs = raw_data.max(axis=1, keepdims=True)\n",
        "sensor_ranges = sensor_maxs - sensor_mins\n",
        "sensor_ranges[sensor_ranges == 0] = 1e-12  # Handle flat channels\n",
        "\n",
        "# Create a normalized version (each sensor scaled independently to 0..1)\n",
        "norm_data = (raw_data - sensor_mins) / sensor_ranges\n",
        "\n",
        "# Compute raw data limits for color mapping\n",
        "global_raw_min = raw_data.min()\n",
        "global_raw_max = raw_data.max()\n",
        "\n",
        "# Define the filtered sensor indices (zero-indexed)\n",
        "filtered_indices = np.array([18, 20, 22, 23, 45, 120, 138, 140, 142, 143, 145,\n",
        "                             146, 147, 149, 175, 176, 177, 179, 180, 198, 271, 272, 275])\n",
        "# (In case any index is out of bounds, restrict to valid indices)\n",
        "filtered_indices = filtered_indices[filtered_indices < n_channels]\n",
        "filtered_raw_min = raw_data[filtered_indices].min() if len(filtered_indices) > 0 else global_raw_min\n",
        "filtered_raw_max = raw_data[filtered_indices].max() if len(filtered_indices) > 0 else global_raw_max\n",
        "\n",
        "# Compute fixed axes ranges from sensor positions.\n",
        "x_range = [sensor_xyz[:, 0].min(), sensor_xyz[:, 0].max()]\n",
        "y_range = [sensor_xyz[:, 1].min(), sensor_xyz[:, 1].max()]\n",
        "z_range = [sensor_xyz[:, 2].min(), sensor_xyz[:, 2].max()]\n",
        "\n",
        "# --- Non-linear transformation function ---\n",
        "def non_linear_transform(x, scale=1e-11):\n",
        "    \"\"\"\n",
        "    Applies a sign-preserving log transform:\n",
        "      transformed = sign(x) * log10(1 + |x|/scale)\n",
        "    \"\"\"\n",
        "    return np.sign(x) * np.log10(1 + np.abs(x)/scale)\n",
        "\n",
        "# --- Jitter duplicates helper ---\n",
        "def jitter_duplicates(xyz, jitter=2e-3):\n",
        "    \"\"\"\n",
        "    Adds a small offset to duplicate sensor coordinates.\n",
        "    Sensors with identical coordinates are slightly displaced so they remain distinct.\n",
        "    \"\"\"\n",
        "    jittered_xyz = xyz.copy()\n",
        "    seen = {}\n",
        "    for i, point in enumerate(xyz):\n",
        "        # Round the coordinates to avoid floating-point precision issues\n",
        "        key = tuple(np.round(point, decimals=6))\n",
        "        if key in seen:\n",
        "            # Calculate an offset based on how many times the coordinate was seen\n",
        "            offset = jitter * seen[key]\n",
        "            jittered_xyz[i] = point + np.array([offset, offset, offset])\n",
        "            seen[key] += 1\n",
        "        else:\n",
        "            seen[key] = 1\n",
        "    return jittered_xyz\n",
        "\n",
        "# --- Utility function to select data based on mode and sensor filter ---\n",
        "def get_current_data(time_idx, mode, sensor_filter):\n",
        "    if mode == 'Normalized':\n",
        "        data = norm_data\n",
        "        cmin, cmax = 0, 1\n",
        "    else:  # Raw mode with non-linear transform\n",
        "        data = non_linear_transform(raw_data, scale=1e-11)\n",
        "        if sensor_filter == 'Mask only':\n",
        "            cmin = non_linear_transform(filtered_raw_min, scale=1e-11)\n",
        "            cmax = non_linear_transform(filtered_raw_max, scale=1e-11)\n",
        "        else:\n",
        "            cmin = non_linear_transform(global_raw_min, scale=1e-11)\n",
        "            cmax = non_linear_transform(global_raw_max, scale=1e-11)\n",
        "\n",
        "    # Determine sensor indices based on the sensor filter.\n",
        "    if sensor_filter == 'Mask only':\n",
        "        indices = filtered_indices\n",
        "    else:\n",
        "        indices = np.arange(n_channels)\n",
        "\n",
        "    current_values = data[indices, time_idx]\n",
        "    # Apply jitter to sensor positions to avoid overlap when coordinates are identical\n",
        "    current_xyz = jitter_duplicates(sensor_xyz[indices])\n",
        "\n",
        "    # Compute per-sensor min and max values for display.\n",
        "    if mode == 'Normalized':\n",
        "        sensor_mins_disp = np.zeros_like(current_values)\n",
        "        sensor_maxs_disp = np.ones_like(current_values)\n",
        "    else:\n",
        "        sensor_mins_disp = non_linear_transform(sensor_mins.squeeze()[indices], scale=1e-11)\n",
        "        sensor_maxs_disp = non_linear_transform(sensor_maxs.squeeze()[indices], scale=1e-11)\n",
        "\n",
        "    return current_xyz, current_values, sensor_mins_disp, sensor_maxs_disp, cmin, cmax\n",
        "\n",
        "# --- Helper function to create hover text ---\n",
        "def create_hover_text(xyz, current_vals, mins, maxs):\n",
        "    texts = []\n",
        "    for i in range(len(current_vals)):\n",
        "        texts.append(f\"X: {xyz[i,0]:.3f}<br>\"\n",
        "                     f\"Y: {xyz[i,1]:.3f}<br>\"\n",
        "                     f\"Z: {xyz[i,2]:.3f}<br>\"\n",
        "                     f\"Current: {current_vals[i]:.3f}<br>\"\n",
        "                     f\"Min: {mins[i]:.3f}<br>\"\n",
        "                     f\"Max: {maxs[i]:.3f}\")\n",
        "    return texts\n",
        "\n",
        "# --- Create the initial plot ---\n",
        "initial_time_idx = 0\n",
        "initial_mode = 'Normalized'\n",
        "initial_filter = 'All'\n",
        "current_xyz, current_values, sensor_mins_disp, sensor_maxs_disp, cmin, cmax = get_current_data(initial_time_idx, initial_mode, initial_filter)\n",
        "hover_text = create_hover_text(current_xyz, current_values, sensor_mins_disp, sensor_maxs_disp)\n",
        "\n",
        "scatter = go.Scatter3d(\n",
        "    x=current_xyz[:, 0],\n",
        "    y=current_xyz[:, 1],\n",
        "    z=current_xyz[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=5,\n",
        "        color=current_values,\n",
        "        colorscale='Viridis',\n",
        "        cmin=cmin,\n",
        "        cmax=cmax,\n",
        "        colorbar=dict(title=\"Amplitude\")\n",
        "    ),\n",
        "    text=hover_text,\n",
        "    hoverinfo='text'\n",
        ")\n",
        "\n",
        "fig = go.FigureWidget(\n",
        "    data=[scatter],\n",
        "    layout=go.Layout(\n",
        "        height=650,\n",
        "        title=f\"LibriBrain MEG data - {initial_mode}, {initial_filter} - Time: {times[initial_time_idx]:.3f}s\",\n",
        "        scene=dict(\n",
        "            xaxis=dict(range=x_range, title=\"X Position\"),\n",
        "            yaxis=dict(range=y_range, title=\"Y Position\"),\n",
        "            zaxis=dict(range=z_range, title=\"Z Position\"),\n",
        "            aspectmode=\"cube\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# --- Create widgets for interactivity ---\n",
        "time_slider = widgets.FloatSlider(\n",
        "    value=times[0],\n",
        "    min=times[0],\n",
        "    max=times[-1],\n",
        "    step=0.004,  # Since polling rate is 250 Hz\n",
        "    continuous_update=True,\n",
        "    readout_format='.3f'\n",
        ")\n",
        "\n",
        "raw_time_input = widgets.FloatText(\n",
        "    value=times[0],\n",
        "    layout=widgets.Layout(width='160px'),\n",
        "    description=\"Time (s):\"\n",
        ")\n",
        "\n",
        "norm_dropdown = widgets.Dropdown(\n",
        "    options=['Normalized', 'Raw'],\n",
        "    value=initial_mode,\n",
        "    description='Value Type:'\n",
        ")\n",
        "\n",
        "filter_dropdown = widgets.Dropdown(\n",
        "    options=['All', 'Mask only'],\n",
        "    value=initial_filter,\n",
        "    style={'description_width': '150px'},\n",
        "    description='Displayed sensors:'\n",
        ")\n",
        "\n",
        "# --- Synchronize the slider and raw time input ---\n",
        "def on_slider_change(change):\n",
        "    new_val = change['new']\n",
        "    if raw_time_input.value != new_val:\n",
        "        raw_time_input.value = new_val\n",
        "\n",
        "def on_raw_time_change(change):\n",
        "    new_val = change['new']\n",
        "    if time_slider.value != new_val:\n",
        "        time_slider.value = new_val\n",
        "\n",
        "time_slider.observe(on_slider_change, names='value')\n",
        "raw_time_input.observe(on_raw_time_change, names='value')\n",
        "\n",
        "# --- Callback to update the plot ---\n",
        "def update_plot(change):\n",
        "    t = time_slider.value\n",
        "    # Find the closest index using the sampling frequency:\n",
        "    time_idx = min(int(round(t * sfreq)), n_timepoints - 1)\n",
        "    mode = norm_dropdown.value\n",
        "    sensor_filter = filter_dropdown.value\n",
        "    current_xyz, current_values, sensor_mins_disp, sensor_maxs_disp, cmin, cmax = get_current_data(time_idx, mode, sensor_filter)\n",
        "    hover_text = create_hover_text(current_xyz, current_values, sensor_mins_disp, sensor_maxs_disp)\n",
        "    with fig.batch_update():\n",
        "        fig.data[0].x = current_xyz[:, 0]\n",
        "        fig.data[0].y = current_xyz[:, 1]\n",
        "        fig.data[0].z = current_xyz[:, 2]\n",
        "        fig.data[0].marker.color = current_values\n",
        "        fig.data[0].marker.cmin = cmin\n",
        "        fig.data[0].marker.cmax = cmax\n",
        "        fig.data[0].text = hover_text\n",
        "        fig.layout.title = f\"LibriBrain MEG data - {mode}, {sensor_filter} - Time: {t:.3f}s\"\n",
        "\n",
        "time_slider.observe(update_plot, names='value')\n",
        "norm_dropdown.observe(update_plot, names='value')\n",
        "filter_dropdown.observe(update_plot, names='value')\n",
        "\n",
        "display(VBox([fig, HBox([raw_time_input, time_slider, norm_dropdown, filter_dropdown])]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro820HGJwXiq"
      },
      "source": [
        "You might have noticed that we have a lot more data points than samples reported by the dataset.\n",
        "\n",
        "This is because the dataset considers a single sample to be the collection of data points reported in the timeframe from `tmin` to `tmax`.\n",
        "In our case, with a sampling rate of 250Hz, `tmin` of 0.0s and `tmax` of 0.8s, we end up with 250 * 0.8 = 200 data points / timesteps per sample.\n",
        "\n",
        "While we're not doing so in this Colab, you can use this to counter some of the imbalance of the data. Using the `oversample_silence_jitter` parameter in the dataset, you can \"shift\" the sampling window slower over periods of silence. A good value we found is 70 steps per sample instead of 200 (`oversample_silence_jitter=70`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgqnVLOv_hoF"
      },
      "source": [
        "#### Labels\n",
        "Next, let's explore the labels. As you can see below, the data includes overlapping phoneme and word labels for each period of heard speech.\n",
        "For the speech detection task, we only care about differentiating between *speech* (time where there is a \"word\" label) and *non-speech* (time where there is a \"silence\" label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2oJBt4N_iRv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def explore_tsv(file_path):\n",
        "    def format_series(series):\n",
        "        if series.dtype in ['float64', 'int64']:\n",
        "            return (f\"Series(dtype={series.dtype}, min={series.min()}, \"\n",
        "                    f\"max={series.max()}, mean={series.mean():.3e}, \"\n",
        "                    f\"std={series.std():.3e})\")\n",
        "        elif series.dtype == 'object':\n",
        "            unique_values = series.nunique()\n",
        "            return (f\"Series(dtype={series.dtype}, unique_values={unique_values})\")\n",
        "        else:\n",
        "            return f\"Series(dtype={series.dtype})\"\n",
        "\n",
        "    print(\"Structure of TSV file:\")\n",
        "    data = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "    print(f\"\\nNumber of rows: {data.shape[0]}\")\n",
        "    print(f\"Number of columns: {data.shape[1]}\")\n",
        "    print(f\"\\nColumns and Data Types:\")\n",
        "    for col in data.columns:\n",
        "        print(f\"  {col}: {format_series(data[col])}\")\n",
        "\n",
        "    print(\"\\nSample Data:\")\n",
        "    print(data.head(n=20))\n",
        "\n",
        "tsv_file_path = f\"{base_path}/data/Sherlock1/derivatives/events/sub-0_ses-1_task-Sherlock1_run-1_events.tsv\"\n",
        "explore_tsv(tsv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ByL6UW_kON"
      },
      "source": [
        "## Understanding the task\n",
        "Now that we understand what the data looks like, let's understand what task we're trying to solve.\n",
        "\n",
        "This year, the LibriBrain competition consists of two tracks:\n",
        "1. **Speech detection**: Classifying which parts of the provided MEG data are speech\n",
        "2. **Phoneme classification**: Predicting which phonemes are heard for the provided MEG data\n",
        "\n",
        "Inspired by [Sutton's Bitter Lesson](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf) (and to keep the competition as accessible as possible), there are two variants for the two tracks:\n",
        "- *Standard*: Training data must be limited to the `Sherlock` data provided in the LibriBrain dataset\n",
        "- *Extended*: Everything goes - you may use whatever data you wish to.\n",
        "\n",
        "#### In this notebook...\n",
        "... we will only work on the *standard* variant of the speech detection task. In fact, we'll only use a portion of the LibriBrain data.\n",
        "\n",
        "As you might expect, we'll be working with two output classes\n",
        "- Speech\n",
        "- Not Speech\n",
        "\n",
        "which means we will be building a binary classifier. Now, let's take a look at what this prediction task looks like in practice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XxHF5JfjfTz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import h5py\n",
        "from mne import create_info\n",
        "\n",
        "\n",
        "def load_meg_data(hdf5_file_path):\n",
        "    with h5py.File(hdf5_file_path, 'r') as f:\n",
        "        raw_data = f['data'][:]\n",
        "        times = f['times'][:]\n",
        "    if len(times) >= 2:\n",
        "        dt = times[1] - times[0]\n",
        "        sfreq = 1.0 / dt\n",
        "    else:\n",
        "        raise ValueError(\"Not enough time points in 'times' to determine sampling frequency.\")\n",
        "    n_channels = raw_data.shape[0]\n",
        "    # Create MNE Info object with default channel names.\n",
        "    channel_names = [f'MEG {i+1:03d}' for i in range(n_channels)]\n",
        "    info = create_info(ch_names=channel_names, sfreq=sfreq, ch_types=['mag'] * n_channels)\n",
        "    return raw_data, info\n",
        "\n",
        "\n",
        "def meg_label_visualization(tsv_data, meg_raw, info, start_time, end_time, title=None, show_phonemes=False, apply_sensor_mask=False):\n",
        "    \"\"\"\n",
        "    Combine MEG data visualization and speech/silence labels.\n",
        "\n",
        "    Parameters:\n",
        "      - tsv_data: DataFrame with timing and labeling information.\n",
        "      - meg_raw: MEG data array (channels x samples).\n",
        "      - info: MNE Info object containing metadata (including sampling frequency).\n",
        "      - start_time, end_time: Time window (in seconds) to visualize.\n",
        "      - title: Optional title for the plots.\n",
        "      - show_phonemes: Whether to show phoneme annotations.\n",
        "      - apply_sensor_mask: force applying the sensor mask;\n",
        "    \"\"\"\n",
        "    # --- Optionally apply sensor mask if using filtered data ---\n",
        "    if apply_sensor_mask:\n",
        "        try:\n",
        "            meg_raw = meg_raw[SENSORS_SPEECH_MASK, :]\n",
        "        except NameError:\n",
        "            print(\"SENSORS_SPEECH_MASK is not defined. Proceeding without sensor mask.\")\n",
        "\n",
        "    # --- Process TSV data to build ground-truth labels ---\n",
        "    tsv_data = tsv_data.copy()\n",
        "    tsv_data['timemeg'] = tsv_data['timemeg'].astype(float)\n",
        "    last_before = tsv_data[tsv_data['timemeg'] < start_time].iloc[-1:]\n",
        "    window_data = tsv_data[(tsv_data['timemeg'] >= start_time) & (tsv_data['timemeg'] <= end_time)]\n",
        "    filtered_data = pd.concat([last_before, window_data]).sort_values('timemeg')\n",
        "    if filtered_data.empty:\n",
        "        filtered_data = pd.DataFrame({'timemeg': [start_time, end_time], 'speech_label': [0, 0]})\n",
        "    filtered_data['speech_label'] = 0\n",
        "    filtered_data.loc[filtered_data['kind'].isin(['word', 'phoneme']), 'speech_label'] = 1\n",
        "\n",
        "    first_value = filtered_data.iloc[0]['speech_label']\n",
        "    plot_times = [start_time]\n",
        "    plot_values = [first_value]\n",
        "    plot_times.extend(filtered_data['timemeg'].tolist())\n",
        "    plot_values.extend(filtered_data['speech_label'].tolist())\n",
        "    plot_times.append(end_time)\n",
        "    plot_values.append(plot_values[-1])\n",
        "    plot_times = np.array(plot_times)\n",
        "    plot_values = np.array(plot_values)\n",
        "\n",
        "    # --- Extract the MEG segment ---\n",
        "    sfreq = info['sfreq']\n",
        "    start_sample = int(start_time * sfreq)\n",
        "    end_sample = int(end_time * sfreq)\n",
        "    meg_segment = meg_raw[:, start_sample:end_sample]  # shape: (channels, samples)\n",
        "    time_points = np.linspace(start_time, end_time, meg_segment.shape[1])\n",
        "\n",
        "    # --- Plotting ---\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(14, 10), gridspec_kw={'height_ratios': [2, 1]}, sharex=True)\n",
        "    meg_title = f\"{title}: MEG Data ({start_time}s to {end_time}s)\" if title else f\"MEG Data ({start_time}s to {end_time}s)\"\n",
        "    axs[0].plot(time_points, meg_segment.T, alpha=0.5)\n",
        "    axs[0].set_title(meg_title)\n",
        "    axs[0].set_ylabel('Amplitude')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    axs[1].plot(plot_times, plot_values, drawstyle='steps-post', label='Speech (1) / Silence (0)',\n",
        "                linewidth=2, color='black')\n",
        "    for _, row in filtered_data.iterrows():\n",
        "        if pd.isna(row['kind']) or row['kind'] == 'silence':\n",
        "            continue\n",
        "        if pd.isna(row['timemeg']) or pd.isna(row['segment']):\n",
        "            continue\n",
        "        try:\n",
        "            if row['kind'] == 'phoneme' and show_phonemes and start_time <= row['timemeg'] <= end_time:\n",
        "                axs[1].text(row['timemeg'], 1.1, str(row['segment']), fontsize=9, rotation=45, ha='center')\n",
        "            elif row['kind'] == 'word' and start_time <= row['timemeg'] <= end_time:\n",
        "                axs[1].text(row['timemeg'], 1.3, str(row['segment']), fontsize=10, rotation=0, ha='center', color='blue')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not plot annotation at time {row['timemeg']}: {e}\")\n",
        "\n",
        "    axs[1].set_ylabel('Speech / Silence')\n",
        "    labels_title = f\"{title}: Labels ({start_time}s to {end_time}s)\" if title else f\"Labels ({start_time}s to {end_time}s)\"\n",
        "    axs[1].set_title(labels_title)\n",
        "    axs[1].set_ylim(-0.2, 1.5)\n",
        "    axs[1].set_xlim(start_time, end_time)\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True)\n",
        "    axs[1].set_xlabel('Time (s)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Define your file paths (ensure that base_path is defined or replace with actual paths)\n",
        "tsv_file_path = f\"{base_path}/data/Sherlock1/derivatives/events/sub-0_ses-1_task-Sherlock1_run-1_events.tsv\"\n",
        "hdf5_file_path = f\"{base_path}/data/Sherlock1/derivatives/serialised/sub-0_ses-1_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5\"\n",
        "\n",
        "# Load data from TSV and HDF5 files\n",
        "tsv_data = pd.read_csv(tsv_file_path, sep='\\t')\n",
        "meg_raw, info = load_meg_data(hdf5_file_path)\n",
        "\n",
        "# Pure speech\n",
        "meg_label_visualization(tsv_data, meg_raw, info, start_time=182, end_time=183, title='Pure speech')\n",
        "\n",
        "# Pure silence\n",
        "meg_label_visualization(tsv_data, meg_raw, info, start_time=1095, end_time=1096, title='Pure silence')\n",
        "\n",
        "# Transition silence -> speech\n",
        "meg_label_visualization(tsv_data, meg_raw, info, start_time=55, end_time=56, title='Transition silence -> speech')\n",
        "\n",
        "# Transition speech -> silence\n",
        "meg_label_visualization(tsv_data, meg_raw, info, start_time=953, end_time=954, title='Transition speech -> silence')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDmnmmWS_wsI"
      },
      "source": [
        "While we are only doing a binary classification (speech/non-speech), there are actually four types of signals that might be interesting to look at:\n",
        "- Pure speech\n",
        "- Pure silence\n",
        "- Transition from silence to speech\n",
        "- Transition from speech to silence\n",
        "\n",
        "Above, you can see examples of those types taken from the training data.\n",
        "\n",
        "Note that you can opt into showing individual phonemes by setting `show_phonemes=True` for each of these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPSL76FE_0uu"
      },
      "source": [
        "## Training a model\n",
        "Now that we understand both the data and the speech detection task, let's go forward with arranging the data and training our speech detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxDVpG6wCwkH"
      },
      "source": [
        "### Preparing the data\n",
        "First, let's set up proper dataloaders with more training data.\n",
        "\n",
        "For the sake of simplicity (and to speed things up), we will only work with the first and second book, \"A Study in Scarlet\" (referred to as `Sherlock1`) and \"The Sign of the Four\" (referred to as `Sherlock2`) in this Colab. For the competition, you can adapt the cell below to load the rest of the dataset, though doing so may not be feasible within the Google Colab free tier.\n",
        "\n",
        "In our case, since `Sherlock1` consists of 12 sessions in total, we'll use\n",
        "- sessions 1 through 10 for training,\n",
        "- session 11 for validation, and\n",
        "- session 12 as a test set.\n",
        "\n",
        "We'll also use all 13 sessions of `Sherlock2` as additional training data.\n",
        "\n",
        "From this point forward, you could theoretically take the sensor data and throw it directly into your favorite model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U22P9xiomhF"
      },
      "outputs": [],
      "source": [
        "from pnpl.datasets import LibriBrainSpeech\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Conditionally set num_workers to avoid multiprocessing issues (try increasing if performance is problematic)\n",
        "num_workers = 2 if in_colab else 0\n",
        "\n",
        "# For training, we'll use sessions 1-10 of Sherlock1 and 1-13 of Sherlock2\n",
        "train_run_keys = [(\"0\",str(i),\"Sherlock1\",\"1\") for i in range(1, 11)] + [(\"0\",str(i),\"Sherlock2\",\"1\") for i in range(1, 13) if i!=2]\n",
        "train_data = LibriBrainSpeech(\n",
        "  data_path=f\"{base_path}/data/\",\n",
        "  include_run_keys = train_run_keys,\n",
        "  tmin=0.0,\n",
        "  tmax=0.8,\n",
        "  preload_files = True\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "\n",
        "# For validation, we'll use session 11 of Sherlock1\n",
        "val_data = LibriBrainSpeech(\n",
        "  data_path=f\"{base_path}/data/\",\n",
        "  include_run_keys=[(\"0\",\"11\",\"Sherlock1\",\"2\")],\n",
        "  standardize=True,\n",
        "  tmin=0.0,\n",
        "  tmax=0.8,\n",
        "  preload_files = True\n",
        ")\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "\n",
        "# For testing, we'll use session 12 of Sherlock1\n",
        "test_data = LibriBrainSpeech(\n",
        "  data_path=f\"{base_path}/data/\",\n",
        "  include_run_keys=[(\"0\",\"12\",\"Sherlock1\",\"2\")],\n",
        "  standardize=True,\n",
        "  tmin=0.0,\n",
        "  tmax=0.8,\n",
        "  preload_files = True\n",
        ")\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "\n",
        "print(\"Number of training samples:\", len(train_data))\n",
        "print(\"Number of validation samples:\", len(val_data))\n",
        "print(\"Number of test samples:\", len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3UwRTsTbrc"
      },
      "source": [
        "In our experience, however, it can pay off to take some time to do additional preparation, so let's do that next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU8s_wRkzeEL"
      },
      "source": [
        "#### Simplifying the prediction task\n",
        "As it stands, the dataset expects the model to predict 200 labels per sample - one for every timestep in the input (remember that each sample is composed of 0.8s of data at 250Hz, each containing the sensor values of 306 sensors at that point in time).\n",
        "\n",
        "Turns out, this is a pretty hard task to learn. So instead, what if the model only had to produce a single prediction per sample — specifically, whether the middle time point in a 200-sample corresponds to speech or not? The input would still be the full 0.8 seconds of MEG data (i.e., 200 time steps), but the model’s output would now be a single label for the central time step. This doesn’t mean we lose the ability to predict labels for all time points. Instead, we can slide a 200-sample “window” across the recording, one timestep at a time, making a prediction for the center of each sample window. In this way, the model still makes predictions for all timesteps, but each one is made with context from both the past and future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "956PXumUyMWU"
      },
      "source": [
        "#### Sensor masking\n",
        "We identified some sensors that provided more activity at relevant frequencies than others during speech periods.\n",
        "In our experience, only including these sensors in training can lead to improved results. These empirical results track with prior research showing these areas as being important for speech processing.\n",
        "Note that you can visualize \"our\" mask in the sensor location visualization above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u6y_WOa0yTi"
      },
      "source": [
        "Take a look at the comparison below - this does look much more managable, right? (Right-click -> New Tab if the text is too small to read)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u3KrCTMqG00"
      },
      "source": [
        "![Task comparison](https://neural-processing-lab.github.io/2025-libribrain-competition/speech-colab-task-comparison.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PWa6oA3fuqx"
      },
      "source": [
        "We've implemented both of these adaptions into the `FilteredDataset` below, which is the dataset we'll use for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESFBWR_jxt9l"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import platform\n",
        "\n",
        "# These are the sensors we identified as being particularly useful\n",
        "SENSORS_SPEECH_MASK = [18, 20, 22, 23, 45, 120, 138, 140, 142, 143, 145,\n",
        "                       146, 147, 149, 175, 176, 177, 179, 180, 198, 271, 272, 275]\n",
        "\n",
        "class FilteredDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        dataset: LibriBrain dataset.\n",
        "        limit_samples (int, optional): If provided, limits the length of the dataset to this\n",
        "                          number of samples.\n",
        "        speech_silence_only (bool, optional): If True, only includes segments that are either\n",
        "                          purely speech or purely silence (with additional balancing).\n",
        "        apply_sensors_speech_mask (bool, optional): If True, applies a fixed sensor mask to the sensor\n",
        "                          data in each sample.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 limit_samples=None,\n",
        "                 disable=False,\n",
        "                 apply_sensors_speech_mask=True):\n",
        "        self.dataset = dataset\n",
        "        self.limit_samples = limit_samples\n",
        "        self.apply_sensors_speech_mask = apply_sensors_speech_mask\n",
        "\n",
        "        # These are the sensors we identified:\n",
        "        self.sensors_speech_mask = SENSORS_SPEECH_MASK\n",
        "\n",
        "        self.balanced_indices = list(range(len(dataset.samples)))\n",
        "        # Shuffle the indices\n",
        "        self.balanced_indices = random.sample(self.balanced_indices, len(self.balanced_indices))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the filtered dataset.\"\"\"\n",
        "        if self.limit_samples is not None:\n",
        "            return self.limit_samples\n",
        "        return len(self.balanced_indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Map index to the original dataset using balanced indices\n",
        "        original_idx = self.balanced_indices[index]\n",
        "        if self.apply_sensors_speech_mask:\n",
        "            sensors = self.dataset[original_idx][0][self.sensors_speech_mask]\n",
        "        else:\n",
        "            sensors = self.dataset[original_idx][0][:]\n",
        "        label_from_the_middle_idx = self.dataset[original_idx][1].shape[0] // 2\n",
        "        return [sensors, self.dataset[original_idx][1][label_from_the_middle_idx]]\n",
        "\n",
        "\n",
        "# Conditionally set num_workers to avoid multiprocessing issues (try increasing if performance is problematic)\n",
        "num_workers = 2 if in_colab else 0\n",
        "\n",
        "print(\"Filtered dataset:\")\n",
        "train_data_filtered = FilteredDataset(train_data)\n",
        "train_loader_filtered = DataLoader(train_data_filtered, batch_size=32, shuffle=True, num_workers=num_workers)\n",
        "print(f\"Train data contains {len(train_data_filtered)} samples\")\n",
        "\n",
        "val_data_filtered = FilteredDataset(val_data)\n",
        "val_loader_filtered = DataLoader(val_data_filtered, batch_size=32, shuffle=False, num_workers=num_workers)\n",
        "print(f\"Validation data contains {len(val_data_filtered)} samples\")\n",
        "\n",
        "test_data_filtered = FilteredDataset(test_data)\n",
        "test_loader_filtered = DataLoader(test_data_filtered, batch_size=32, shuffle=False, num_workers=num_workers)\n",
        "print(f\"Test data contains {len(test_data_filtered)} samples\\n\")\n",
        "\n",
        "# Let's look at the first batch:\n",
        "first_batch = next(iter(train_loader_filtered))\n",
        "inputs, labels = first_batch\n",
        "print(\"Batch input shape:\", inputs.shape)\n",
        "print(\"Batch label shape:\", labels.shape)\n",
        "\n",
        "first_input = inputs[0]\n",
        "first_label = labels[0]\n",
        "print(\"\\nSingle sample input shape:\", first_input.shape)\n",
        "print(\"Single sample label is just a single value now!\")\n",
        "print(\"\\nFirst sample input:\", first_input)\n",
        "print(\"First sample label:\", first_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb2rddzNELjt"
      },
      "source": [
        "### Actual training loop\n",
        "With all that out of the way, let's actually train the model! First, we define the actual model architecture. We will use a 1 Dimensional Convolutional layer - to process the spatial relations between the sensors, followed by a 2 layer LSTM to process the temporal dynamics within each segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZdhuOMCoShT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SpeechModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        input_dim (int): Number of channels/features in the input tensor (usually SENSORS_SPEECH_MASK)\n",
        "        model_dim (int): Dimensionality for the intermediate model representation.\n",
        "        dropout_rate (float, optional): Dropout probability applied after convolutional and LSTM layers.\n",
        "        lstm_layers (int, optional): Number of layers in the LSTM module.\n",
        "        bi_directional (bool, optional): If True, uses a bidirectional LSTM; otherwise, a unidirectional LSTM.\n",
        "        batch_norm (bool, optional): Indicates whether to use batch normalization.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, model_dim, dropout_rate=0.3, lstm_layers = 1, bi_directional = False, batch_norm=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=input_dim,\n",
        "            out_channels=model_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=model_dim) if batch_norm else nn.Identity()\n",
        "        self.conv_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=model_dim,\n",
        "            hidden_size=model_dim,\n",
        "            num_layers=self.lstm_layers,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True,\n",
        "            bidirectional=bi_directional\n",
        "        )\n",
        "        self.lstm_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.speech_classifier = nn.Linear(model_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.conv_dropout(x)\n",
        "        # LSTM expects (batch, seq_len, input_size)\n",
        "        output, (h_n, c_n) = self.lstm(x.permute(0, 2, 1))\n",
        "        last_layer_h_n = h_n\n",
        "        if self.lstm_layers > 1:\n",
        "            # handle more than one layer\n",
        "            last_layer_h_n = h_n[-1, :, :]\n",
        "            last_layer_h_n = last_layer_h_n.unsqueeze(0)\n",
        "        output = self.lstm_dropout(last_layer_h_n)\n",
        "        output = output.flatten(start_dim=0, end_dim=1)\n",
        "        x = self.speech_classifier(output)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oC6JpBU_r_6"
      },
      "source": [
        "Next, let's wrap the model in a [LightningModule](https://lightning.ai/docs/pytorch/LTS/common/lightning_module.html) to save ourselves from some boilerplate.\n",
        "This is also where we define our loss function (BCE with label smoothing) and optimizer (AdamW)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8LRIWURXb5n"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "from sklearn.metrics import roc_curve, auc, balanced_accuracy_score, jaccard_score\n",
        "from torchmetrics import Precision, Recall, F1Score\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "import numpy as np\n",
        "from torchmetrics.functional import recall\n",
        "\n",
        "\n",
        "class SpeechClassifier(L.LightningModule):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        input_dim (int): Number of input channels/features. This is passed to the underlying SpeechModel.\n",
        "        model_dim (int): Dimensionality of the intermediate model representation.\n",
        "        learning_rate (float, optional): Learning rate for the optimizer.\n",
        "        weight_decay (float, optional): Weight decay for the optimizer.\n",
        "        batch_size (int, optional): Batch size used during training and evaluation.\n",
        "        dropout_rate (float, optional): Dropout probability applied after convolutional and LSTM layers.\n",
        "        smoothing (float, optional): Label smoothing factor applied in the BCEWithLogits loss.\n",
        "        pos_weight (float, optional): Weight for the positive class in the BCEWithLogits loss.\n",
        "        batch_norm (bool, optional): Indicates whether to use batch normalization.\n",
        "        lstm_layers (int, optional): Number of layers in the LSTM module within the SpeechModel.\n",
        "        bi_directional (bool, optional): If True, uses a bidirectional LSTM in the SpeechModel; otherwise, uses a unidirectional LSTM.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, model_dim, learning_rate=1e-3, weight_decay=0.01, batch_size=32, dropout_rate=0.3, smoothing=0.1, pos_weight = 1.0 , batch_norm = False, lstm_layers = 1, bi_directional = False):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.model = SpeechModel(input_dim, model_dim, dropout_rate=dropout_rate, lstm_layers=lstm_layers, bi_directional=bi_directional, batch_norm=batch_norm)\n",
        "\n",
        "        self.loss_fn = BCEWithLogitsLossWithSmoothing(smoothing=smoothing, pos_weight = pos_weight)\n",
        "\n",
        "        self.val_step_outputs = []\n",
        "        self.test_step_outputs = {}\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "            return self.model(x)\n",
        "\n",
        "    def _shared_eval_step(self, batch, stage):\n",
        "        x = batch[0]\n",
        "        y = batch[1] # (batch, seq_len)\n",
        "\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.unsqueeze(1).float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "        y_probs = probs.detach().cpu()\n",
        "\n",
        "        y_true = batch[1].detach().cpu()\n",
        "        meg = x.detach().cpu()\n",
        "\n",
        "        self.log(f'{stage}_loss', loss, on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._shared_eval_step(batch, \"train\")\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._shared_eval_step(batch, \"val\")\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x = batch[0]\n",
        "        y = batch[1]  # (batch, seq_len)\n",
        "\n",
        "        # ugly, taking care of only one label\n",
        "        if len(y.shape) != 1:\n",
        "            y = y.flatten(start_dim=0, end_dim=1).view(-1, 1)  # (batch, seq_len) -> (batch * seq_len, 1)\n",
        "        else:\n",
        "            y = y.unsqueeze(1)\n",
        "\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y.float())\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        # Append data to the defaultdict\n",
        "        # Ensure keys exist before appending\n",
        "        if \"y_probs\" not in self.test_step_outputs:\n",
        "            self.test_step_outputs[\"y_probs\"] = []\n",
        "        if \"y_true\" not in self.test_step_outputs:\n",
        "            self.test_step_outputs[\"y_true\"] = []\n",
        "        if \"meg\" not in self.test_step_outputs:\n",
        "            self.test_step_outputs[\"meg\"] = []\n",
        "\n",
        "        # Append data\n",
        "        if y.shape[-1] != 1:\n",
        "            self.test_step_outputs[\"y_probs\"].extend(\n",
        "                probs.detach().view(x.shape[0], x.shape[-1]).cpu())  # (batch, seq_len)\n",
        "        else:\n",
        "            self.test_step_outputs[\"y_probs\"].extend(\n",
        "                probs.detach().view(x.shape[0], 1).cpu())  # (batch, seq_len)\n",
        "\n",
        "        self.test_step_outputs[\"y_true\"].extend(batch[1].detach().cpu())  # (batch, seq_len)\n",
        "        self.test_step_outputs[\"meg\"].extend(x.detach().cpu())  # MEG data (batch, channels, seq_len)\n",
        "\n",
        "        return self._shared_eval_step(batch, \"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "class BCEWithLogitsLossWithSmoothing(nn.Module):\n",
        "    def __init__(self, smoothing=0.1, pos_weight = 1.0):\n",
        "        \"\"\"\n",
        "        Binary Cross-Entropy Loss with Deterministic Label Smoothing.\n",
        "\n",
        "        Parameters:\n",
        "            smoothing (float): Smoothing factor. Must be between 0 and 1.\n",
        "            pos_weight (float): Weight for the positive class.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        target = target.float()  # Ensure target is a float tensor\n",
        "        target_smoothed = target * (1 - self.smoothing) + self.smoothing * 0.5\n",
        "        return self.bce_loss(logits, target_smoothed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I5mIsOi_yTg"
      },
      "source": [
        "Finally, here's the training loop. We'll either use a basic CSVLogger when running locally or the built-in Tensorboard in Colab for logging to keep things self-contained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpGlWQUC_zrh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import lightning as L\n",
        "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
        "from lightning.pytorch.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Setup paths for logs and checkpoints\n",
        "LOG_DIR = f\"{base_path}/lightning_logs\"\n",
        "CHECKPOINT_PATH = f\"{base_path}/models/speech_model.ckpt\"\n",
        "\n",
        "# Minimal logging setup\n",
        "logger = CSVLogger(\n",
        "    save_dir=LOG_DIR,\n",
        "    name=\"\",\n",
        "    version=None,\n",
        ")\n",
        "\n",
        "if in_colab:  # In Colab, we use the built-in Tensorboard setup\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=LOG_DIR,\n",
        "        name=\"\",\n",
        "        version=None,\n",
        "        default_hp_metric=True\n",
        "    )\n",
        "    if not os.path.exists(LOG_DIR):\n",
        "        os.makedirs(LOG_DIR)\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir $LOG_DIR\n",
        "\n",
        "\n",
        "# Set a fixed seed for reproducibility\n",
        "L.seed_everything(42)\n",
        "\n",
        "# Initialize the SpeechClassifier model\n",
        "# Note: Feel free to play around with the hyperparameters here!\n",
        "model = SpeechClassifier(\n",
        "    input_dim=len(SENSORS_SPEECH_MASK),\n",
        "    model_dim=100,\n",
        "    learning_rate=1e-3,\n",
        "    dropout_rate=0.5,\n",
        "    lstm_layers=2,\n",
        "    weight_decay=0.01,\n",
        "    batch_norm=False,\n",
        "    bi_directional=False\n",
        ")\n",
        "\n",
        "# Log Hyperparameters\n",
        "logger.log_hyperparams(model.hparams)\n",
        "\n",
        "# Optional: Early stopping\n",
        "#           In our testing, we experienced a lot of issues with overfitting.\n",
        "#           We resolve this by stopping training if validation loss stops going down\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.00,\n",
        "    patience=10,\n",
        "    verbose=True,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = L.Trainer(\n",
        "    devices=\"auto\",\n",
        "    max_epochs=15,\n",
        "    logger=logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[early_stopping_callback]\n",
        ")\n",
        "\n",
        "# Actually train the model\n",
        "trainer.fit(model, train_loader_filtered, val_loader_filtered)\n",
        "# Save trained model weights\n",
        "trainer.save_checkpoint(CHECKPOINT_PATH)\n",
        "# Test trained model\n",
        "trainer.test(model, test_loader_filtered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xru-mbgP7nam"
      },
      "source": [
        "If you instead wish to load pretrained model weights, you can do so here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPebv5Sk5YTB"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "\n",
        "\n",
        "# Setup checkpoint path\n",
        "CHECKPOINT_PATH = f\"{base_path}/models/speech_model.ckpt\"\n",
        "\n",
        "# # Optional: Download the model\n",
        "# model_url = \"https://neural-processing-lab.github.io/2025-libribrain-competition/speech_model.ckpt\"\n",
        "# response = requests.get(model_url)\n",
        "# os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "# with open(CHECKPOINT_PATH, \"wb\") as f:\n",
        "#     f.write(response.content)\n",
        "# print(\"Download of model checkpoint complete.\")\n",
        "\n",
        "# Set a fixed seed for reproducibility (just in case)\n",
        "L.seed_everything(42)\n",
        "\n",
        "# Load the SpeechClassifier model from checkpoint\n",
        "model = SpeechClassifier.load_from_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    input_dim=len(SENSORS_SPEECH_MASK),\n",
        "    model_dim=100,\n",
        "    learning_rate=1e-3,\n",
        "    dropout_rate=0.5,\n",
        "    lstm_layers=2,\n",
        "    weight_decay=0.01,\n",
        "    batch_norm=False,\n",
        "    bi_directional=False\n",
        ")\n",
        "\n",
        "# Initialize trainer and test loaded model\n",
        "trainer = L.Trainer(devices=\"auto\")\n",
        "trainer.test(model, test_loader_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glAsKeEx_-qr"
      },
      "source": [
        "## Evaluating the model\n",
        "You've now trained your model - congratulations! Let's check out the test accuracy on unseen data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzqT-uLsAC5v"
      },
      "outputs": [],
      "source": [
        "trainer.test(model, dataloaders=test_loader_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-6cYRZVVM85"
      },
      "source": [
        "Well, that sounds good but doesn't really tell us anything, so let's take a deeper look at the model's performance. We'll start by running some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tHFThUuMsF3"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_y_true = []\n",
        "all_y_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # # Uncomment to compute statistics on training (might be slow!)\n",
        "    # for batch in train_loader_filtered:\n",
        "    #     x, y = batch\n",
        "    #     logits = model(x)\n",
        "    #     probs = torch.sigmoid(logits)\n",
        "\n",
        "    #     all_y_true.append(y)\n",
        "    #     all_y_probs.append(probs)\n",
        "\n",
        "    # Uncomment to compute statistics on validation set\n",
        "    for batch in val_loader_filtered:\n",
        "        x, y = batch\n",
        "        logits = model(x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        all_y_true.append(y)\n",
        "        all_y_probs.append(probs)\n",
        "\n",
        "    # Uncomment to compute statistics on test set\n",
        "    # for batch in test_loader_filtered:\n",
        "    #     x, y = batch\n",
        "    #     logits = model(x)\n",
        "    #     probs = torch.sigmoid(logits)\n",
        "\n",
        "    #     all_y_true.append(y)\n",
        "    #     all_y_probs.append(probs)\n",
        "\n",
        "\n",
        "y_true = torch.cat(all_y_true, dim=0)\n",
        "y_probs = torch.cat(all_y_probs, dim=0)\n",
        "\n",
        "print(\"Ready to compute statistics with \" + str(len(y_true)) + \" samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyNQJagMPnZf"
      },
      "source": [
        "Let's plot [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. Ideally, we want this graph to immediately rise to (0/1) at the start of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzHhIbBUM3Fd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_auc_roc(labels, probs, title=\"ROC Curve\"):\n",
        "    \"\"\"\n",
        "    Generates an AUC-ROC plot.\n",
        "\n",
        "    Args:\n",
        "        labels (torch.Tensor or np.ndarray): Ground truth binary labels (shape: [num_segments, sequence_length]).\n",
        "        probs (torch.Tensor or np.ndarray): Predicted probabilities for the positive class (shape: [num_segments, sequence_length]).\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    # Convert to NumPy if necessary\n",
        "    if not isinstance(labels, np.ndarray):\n",
        "        labels = labels.cpu().numpy()\n",
        "    if not isinstance(probs, np.ndarray):\n",
        "        probs = probs.cpu().numpy()\n",
        "\n",
        "    # Flatten the data to treat all predictions equally\n",
        "    labels_flat = labels.flatten()\n",
        "    probs_flat = probs.flatten()\n",
        "\n",
        "    # Compute ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(labels_flat, probs_flat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"AUC = {roc_auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "\n",
        "    return fig, roc_auc\n",
        "\n",
        "\n",
        "fig, roc_auc = plot_auc_roc(y_true, y_probs)\n",
        "plt.show()\n",
        "print(\"\\nAUC is \" + str(roc_auc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5CHypnMivT4"
      },
      "source": [
        "\n",
        "Note that even poor models may look okay due to the imbalance of the dataset. Let's take a look at our models predictions in a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b2U3L9M8mGV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "\n",
        "def plot_confusion_matrix_1s_0s(labels, preds):\n",
        "    labels = labels.flatten()\n",
        "    preds = preds.flatten()\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(labels, preds, labels=[0, 1])\n",
        "\n",
        "    # Create a binary matrix indicating correct predictions (diagonals) as 1 and incorrect as 0\n",
        "    cell_type = np.zeros_like(cm)\n",
        "    for i in range(cm.shape[0]):\n",
        "        cell_type[i, i] = 1\n",
        "\n",
        "    # Define a custom colormap: 0 -> red (incorrect), 1 -> blue (correct)\n",
        "    cmap = ListedColormap(['red', 'blue'])\n",
        "\n",
        "    # Plot using matplotlib's imshow\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    ax.imshow(cell_type, cmap=cmap, vmin=-0.5, vmax=1.5)\n",
        "\n",
        "    # Overlay the confusion matrix numbers on top of the colored cells\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, cm[i, j],\n",
        "                    ha='center', va='center', color='white', fontsize=16)\n",
        "\n",
        "    # Set tick labels\n",
        "    ax.set_xticks(np.arange(2))\n",
        "    ax.set_yticks(np.arange(2))\n",
        "    ax.set_xticklabels([\"Predicted 0\", \"Predicted 1\"])\n",
        "    ax.set_yticklabels([\"True 0\", \"True 1\"])\n",
        "    ax.set_xlabel(\"Predicted Label\")\n",
        "    ax.set_ylabel(\"True Label\")\n",
        "    ax.set_title(\"Confusion Matrix\")\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "# Assuming your test_step_outputs dict has been populated from trainer.test(model, test_loader)\n",
        "# Convert the collected outputs into numpy arrays and flatten them if needed\n",
        "y_true = np.array(y_true)\n",
        "y_probs = np.array(y_probs)\n",
        "\n",
        "# Convert probabilities to binary predictions using a threshold of 0.5\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "# Use the provided function to plot the confusion matrix\n",
        "fig = plot_confusion_matrix_1s_0s(y_true, y_pred)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "print(\"\\nIoU (Jaccard Index) is \" + str(iou))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDGZ8LLzAMvb"
      },
      "source": [
        "## That's it! 🥳\n",
        "Thanks for taking the time to look at and/or participate in our competition. If this caught your interest, you might also want to take a look at the more advanced version of the task, focussed on Phoneme Classification - you can find the corresponding Colab [here](https://neural-processing-lab.github.io/2025-libribrain-competition/links/phoneme-colab). If you have any open questions, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord). Ready to submit your model? You might want to take a look at the [submission tutorial](https://neural-processing-lab.github.io/2025-libribrain-competition/links/submission-colab) or the [LibriBrain competition website](https://neural-processing-lab.github.io/2025-libribrain-competition)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}