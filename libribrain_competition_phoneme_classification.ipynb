{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zpsheldon/meg-neural-decoding/blob/main/libribrain_competition_phoneme_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBdHU3xXe3xY"
      },
      "source": [
        "# üçç LibriBrain Competition: Advanced (Phoneme Classification)\n",
        "Welcome! This Colab is the starting point for the second track of the LibriBrain competition hosted by the [PNPL](https://ori.ox.ac.uk/labs/pnpl/) at NeurIPS 2025. For a more basic introduction, you might prefer to take a look at the [Speech Detection variant](https://neural-processing-lab.github.io/2025-libribrain-competition/links/speech-colab) first, which includes a more comprehensive introduction and a slightly simpler task.\n",
        "\n",
        "The following notebook will walk you through\n",
        "1. setting up all necessary dependencies,\n",
        "2. downloading training data, and\n",
        "3. training a minimal model\n",
        "\n",
        "It is fully functional in the Colab Free Tier, though training will of course be faster with more GPU horsepower. With default settings on a `T4` instance, the main training run should take no more than 45 minutes. If you want to speed up model training, make sure you are on a GPU runtime by clicking Runtime -> Change runtime type. TPU acceleration is currently not supported.\n",
        "\n",
        "In case of any questions or problems, please get in touch through [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord).\n",
        "\n",
        "‚ö†Ô∏è **Note**: We have only comprehensively validated the notebook to work on Colab and Unix. Your experience in other environments (e.g., Windows) may vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su09-lYagdyt"
      },
      "source": [
        "## Setting up dependencies\n",
        "Run the code below *as is*. It will download all required dependencies, including our own [PNPL](https://pypi.org/project/pnpl/) package. On Windows, you might have to restart your Kernel after the installation has finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF9GYq1rYGQ5"
      },
      "outputs": [],
      "source": [
        "# Install additional dependencies\n",
        "%pip install -q mne_bids lightning torchmetrics scikit-learn plotly ipywidgets pnpl\n",
        "\n",
        "# Set up base path for dataset and related files (base_path is assumed to be set in the cells below!)\n",
        "base_path = \"./libribrain\"\n",
        "try:\n",
        "    import google.colab  # This module is only available in Colab.\n",
        "    in_colab = True\n",
        "    base_path = \"/content\"  # This is the folder displayed in the Colab sidebar\n",
        "except ImportError:\n",
        "    in_colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6AWehcygrCg"
      },
      "source": [
        "## Preparing the dataset\n",
        "The code below will automatically download the training data. For our reference model, we'll only use Sherlock1 sessions 1-11 as training data. For your implementation, you may want to set `partition=\"train\"` instead of manually providing run keys. This will automatically load all available training data (Sherlock Books 1-7) - an order of magnitude more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m0Z05HnYKp3"
      },
      "outputs": [],
      "source": [
        "from pnpl.datasets import LibriBrainPhoneme\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "train_dataset = LibriBrainPhoneme(\n",
        "  data_path=f\"{base_path}/data/\",\n",
        "  include_run_keys = [(\"0\",str(i),\"Sherlock1\",\"1\") for i in range(1, 10)],\n",
        "  tmin=0.0,\n",
        "  tmax = 0.5\n",
        "  )\n",
        "\n",
        "\n",
        "val_dataset = LibriBrainPhoneme(\n",
        "  data_path=f\"{base_path}/data/\",\n",
        "  partition=\"validation\",  # this is equal to `include_run_keys = [(\"0\",11,\"Sherlock1\",\"1\")]`\n",
        "  tmin=0.0,\n",
        "  tmax = 0.5\n",
        ")\n",
        "\n",
        "\n",
        "test_dataset = LibriBrainPhoneme(\n",
        "  data_path=f\"{base_path}/data/\", # this is equal to `include_run_keys = [(\"0\",12,\"Sherlock1\",\"1\")]\n",
        "  partition=\"test\",\n",
        "  tmin=0.0,\n",
        "  tmax = 0.5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E6O9UU0Nhu9"
      },
      "source": [
        "The minimal example above is completely sufficient for you to get started. If you want to play around, here are all the options you _could_ set:\n",
        "- `data_path`: Path where you wish to store the dataset. The local dataset structure will follow the same BIDS-like structure as the HuggingFace repo:\n",
        "- `partition`: Convenient shortcut to specify train/validation/test split. Options: \"train\", \"validation\", \"test\"\n",
        "- `label_type`: Type of labels to return. Options: \"phoneme\" (e.g., 'aa', 'ae', 'ah', etc.) or \"voicing\": Return voicing labels derived from phonemes indicating voiced. See https://en.wikipedia.org/wiki/Voice_(phonetics)\n",
        "- `preprocessing_str`: By default, we expect files with preprocessing string \"bads+headpos+sss+notch+bp+ds\". This indicates the preprocessing we have done (check Speech Detection tutorial for details)\n",
        "- `tmin`: Start time of the sample in seconds relative to phoneme onset. For a phoneme at time T, you grab MEG data from T + tmin up to T + tmax. Default: 0.0\n",
        "- `tmax`: End time of the sample in seconds relative to phoneme onset. The number of timepoints per sample = int((tmax - tmin) * sfreq) where sfreq=250Hz. Default: 0.5 (this means each segment is made up of 125 samples)\n",
        "- `include_run_keys`: List of specific sessions to include. Format per session: ('0', '1', 'Sherlock1', '1') = Subject 0, Session 1, Task Sherlock1, Run 1.\n",
        "- `exclude_run_keys`: List of sessions to exclude (same format as include_run_keys).\n",
        "- `exclude_tasks`: List of task names to exclude (e.g., ['Sherlock1']).\n",
        "- `standardize`: Whether to z-score normalize each channel's MEG data using mean and std computed across all included runs. Default: True\n",
        "- `clipping_boundary`: If specified, clips all MEG values to [-clipping_boundary, clipping_boundary]. Default: 10\n",
        "- `channel_means` and `channel_stds`: Pre-computed channel means and stds for standardization. If provided, these will be used instead of computing from the dataset.\n",
        "- `include_info`: Whether to include additional info dict in each sample containing dataset name, subject, session, task, run, onset time, and full phoneme label (including word position indicators). Default: False\n",
        "- `preload_files`: Whether to \"eagerly\" download all dataset files from HuggingFace when the dataset object is created (True; default) or \"lazily\" download files on demand (False).\n",
        "- `download`: Whether to download files from HuggingFace if not found locally (True; default) or throw an error if files are missing locally (False)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdc0MkdFPEP_"
      },
      "source": [
        "## The task: Phoneme Classification\n",
        "Before we go any further, let's make sure we understand what we're actually trying to accomplish. In contrast to the earlier Speech Detection task, where we just tried to figure out if our participant was listening to speech or not (binary classification), we're now trying to figure out which sounds (= [phonemes](https://en.wikipedia.org/wiki/Phoneme)) they were listening to. Specifically,\n",
        "- each sample loaded by `LibriBrainPhoneme` corresponds to a \"phoneme event\" (i.e., a phoneme being heard), and\n",
        "- each label/prediction corresponds to one of 39 phonemes (we used the ARPAbet phonemes classification http://speech.cs.cmu.edu/cgi-bin/cmudict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyFms5pXPVFD"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "from pnpl.datasets import LibriBrainPhoneme\n",
        "\n",
        "print(\"Let's look at our train_dataset in more detail:\")\n",
        "print(f\"Dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Phoneme classes: {len(train_dataset.labels_sorted)}\")\n",
        "print()\n",
        "\n",
        "# Let's look at one sample\n",
        "sample_data, label_id = train_dataset[0]\n",
        "# We can map the label ID to the actual phoneme it represents:\n",
        "phoneme = train_dataset.id_to_phoneme[label_id.item()]\n",
        "\n",
        "print(f\"SAMPLE 0:\")\n",
        "# Note: The shape of the sample depends on the values you set for tmin and tmax - we're using default values (tmin=0, tmax=0.5)\n",
        "print(f\"  Data shape: {sample_data.shape} (306 MEG channels, by default: 125 samples = 0.5 seconds)\")\n",
        "print(f\"  Label ID: {label_id.item()}\")\n",
        "print(f\"  Phoneme: '{phoneme}'\")\n",
        "print(f\"  Data range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\n",
        "print()\n",
        "\n",
        "# Show phoneme mappings\n",
        "print(f\"PHONEME MAPPINGS:\")\n",
        "for i, phoneme in enumerate(train_dataset.labels_sorted):\n",
        "    print(f\"  ID {i}: '{phoneme}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g79wqtgQWaj9"
      },
      "source": [
        "## Important: Signal averaging the training data\n",
        "While we could now train our model with the above dataloaders, the signal-to-noise ratio of a single sample is not high enough for reliable classifcation.\n",
        "Therefore, for this years competition, we will only evaluate on samples composed of 100 averaged instances of the same phoneme, with the exception of some of the rarer phonemes, for which less than 100 individual samples were available in the holdout. You can expect the distribution of phonemes in the portion of the holdout relevant for final evaluation to be similar to the validation/test split.\n",
        "\n",
        "Averaging brain signal with repeated stimuli (usally refered to as \"Signal Averaging\" or \"Event-related potential\" in the context of Neuroscience and Electrophysiology) is a common method for improving SNR. Lets explore this method effect on SNR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GepKXePAFNvp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from pnpl.datasets import LibriBrainPhoneme\n",
        "\n",
        "visualization_dataset = LibriBrainPhoneme(\n",
        "    data_path=f\"{base_path}/data/\",\n",
        "    tmin=0.0,\n",
        "    tmax=0.5,\n",
        "    standardize=True,\n",
        "    include_run_keys=[('0', '1', 'Sherlock1', '1')],\n",
        ")\n",
        "\n",
        "# Collect samples for phoneme 'aa'\n",
        "phoneme_id = visualization_dataset.phoneme_to_id['aa']\n",
        "samples = []\n",
        "for i in range(min(len(visualization_dataset), 2000)):\n",
        "    data, label = visualization_dataset[i]\n",
        "    if label.item() == phoneme_id and len(samples) < 15:\n",
        "        samples.append(data)\n",
        "\n",
        "stacked_samples = torch.stack(samples)\n",
        "averaged_signal = stacked_samples.mean(dim=0)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "time = np.linspace(0, 0.3, stacked_samples.shape[2])\n",
        "n_channels_show = 5\n",
        "n_samples_show = 8\n",
        "\n",
        "for ch in range(n_channels_show):\n",
        "    offset = ch * 3\n",
        "    # Plot multiple individual samples (chaotic)\n",
        "    for i in range(n_samples_show):\n",
        "        ax.plot(time, stacked_samples[i, ch, :] + offset, alpha=0.4,\n",
        "                color='lightblue', linewidth=1)\n",
        "    # Plot averaged signal (clean)\n",
        "    ax.plot(time, averaged_signal[ch, :] + offset,\n",
        "            color='darkred', linewidth=2.5)\n",
        "\n",
        "ax.set_title('Signal Averaging: Some channels for phoneme /aa/')\n",
        "ax.set_xlabel('Time (seconds)')\n",
        "ax.set_ylabel('Channel')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwa_wqZ6D41V"
      },
      "source": [
        "As you can see above, averaging the signals of multiple instances of the same phoneme in the training data is sufficient filter out a lot of the noise. This is called [signal averaging](https://en.wikipedia.org/wiki/Signal_averaging). As part of the `pnpl` library, we provide the `GroupedDataset` class to easily  apply this transformation. Let's apply it to our train dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PShBXiGyFHcE"
      },
      "outputs": [],
      "source": [
        "from pnpl.datasets import GroupedDataset\n",
        "\n",
        "\n",
        "# Options:\n",
        "# - `original_dataset`: The original dataset to group\n",
        "# - `grouped_samples`: How many samples of each phoneme to group (default: 10)\n",
        "# - `drop_remaining`: Whether to drop the last group if it is incomplete (default: False)\n",
        "# - `shuffle`: Whether to shuffle the samples (default: False)\n",
        "# - `average_grouped_samples`: Whether to average the grouped samples (default: True)\n",
        "averaged_train_dataset = GroupedDataset(train_dataset, grouped_samples = 100)\n",
        "\n",
        "# As expected, averaging 100 phonemes into a single sample (while keeping the incomplete final groups) reduces the total number of samples by just under 100x:\n",
        "print(\"Single samples: \", len(train_dataset))\n",
        "print(\"Averaged samples: \", len(averaged_train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9kFxp1GLLNX"
      },
      "source": [
        "Keep in mind that while we will evaluate the results on averging of 100 instances, we invite participants to find the optimal value for training a phoneme model. We will stick with grouping 100 samples in this notebook, but you may wish to try other options!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSTwRyOFgx6P"
      },
      "source": [
        "## Defining the model\n",
        "Now that we have explored the training data in detail, this is the model architecture we'll use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjSoKxIWYBSV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import lightning as L\n",
        "from torch import nn\n",
        "from torchmetrics import F1Score\n",
        "\n",
        "# Basic LightningModule\n",
        "class PhonemeClassificationModel(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(306, 128, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16000, 39)\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.f1_macro = F1Score(num_classes=39, average='macro', task=\"multiclass\")\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        f1_macro = self.f1_macro(y_hat, y)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_f1_macro', f1_macro)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        f1_macro = self.f1_macro(y_hat, y)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_f1_macro', f1_macro, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMl25Lpg3Ak"
      },
      "source": [
        "## Actually training\n",
        "The code below will train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7keM-qrpYBSW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
        "\n",
        "# Setup paths for logs and checkpoints\n",
        "LOG_DIR = f\"{base_path}/lightning_logs\"\n",
        "CHECKPOINT_PATH = f\"{base_path}/models/phoneme_model.ckpt\"\n",
        "\n",
        "# Minimal logging setup\n",
        "logger = CSVLogger(\n",
        "    save_dir=LOG_DIR,\n",
        "    name=\"\",\n",
        "    version=None,\n",
        ")\n",
        "if in_colab:  # In Colab, we use the built-in Tensorboard setup\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=LOG_DIR,\n",
        "        name=\"\",\n",
        "        version=None,\n",
        "        default_hp_metric=True\n",
        "    )\n",
        "    if not os.path.exists(LOG_DIR):\n",
        "        os.makedirs(LOG_DIR)\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir $LOG_DIR\n",
        "\n",
        "# Set a fixed seed for reproducibility\n",
        "L.seed_everything(42)\n",
        "\n",
        "# Conditionally set num_workers to avoid multiprocessing issues (try increasing if performance is problematic)\n",
        "num_workers = 2 if in_colab else 0\n",
        "\n",
        "# Configure data loaders\n",
        "train_dataloader = DataLoader(averaged_train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# Initialize the PhonemeClassificationModel model\n",
        "model = PhonemeClassificationModel()\n",
        "\n",
        "# Log Hyperparameters (these will be empty be default!)\n",
        "logger.log_hyperparams(model.hparams)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = L.Trainer(\n",
        "    devices=\"auto\",\n",
        "    max_epochs=15,\n",
        "    logger=logger,\n",
        "    enable_checkpointing=True,\n",
        ")\n",
        "\n",
        "# Actually train the model\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_checkpoint(CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-aAOoAog74h"
      },
      "source": [
        "## Validating our results\n",
        "Let's look at how our model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7GLTlBeYBSW"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import F1Score\n",
        "\n",
        "def validate(val_loader, module, labels):\n",
        "    disp_labels = labels\n",
        "    module.eval()\n",
        "    predicted_phonemes = []\n",
        "    true_phonemes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            x, y = batch\n",
        "            x = x.to(module.device)\n",
        "            y = y.to(module.device)\n",
        "            outputs = module(x)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            predicted_phonemes.extend(preds)\n",
        "            true_phonemes.extend(y)\n",
        "\n",
        "    true_phonemes = torch.stack(true_phonemes)\n",
        "    predicted_phonemes = torch.stack(predicted_phonemes)\n",
        "\n",
        "    f1_macro = F1Score(task=\"multiclass\", average=\"macro\",\n",
        "                       num_classes=len(disp_labels)).to(module.device)\n",
        "\n",
        "    random_preds = torch.randint(\n",
        "        0, len(disp_labels), (len(true_phonemes),), device=module.device)\n",
        "\n",
        "    random_f1_macro = f1_macro(\n",
        "        random_preds, true_phonemes)\n",
        "\n",
        "    f1_macro = f1_macro(predicted_phonemes, true_phonemes)\n",
        "\n",
        "\n",
        "    binary_f1 = F1Score(task=\"binary\").to(module.device)\n",
        "\n",
        "    classes = torch.arange(len(disp_labels))\n",
        "    f1_by_class = []\n",
        "    random_f1_by_class = []\n",
        "    for c in classes:\n",
        "        class_preds = predicted_phonemes == c\n",
        "        class_targets = true_phonemes == c\n",
        "        class_f1 = binary_f1(class_preds, class_targets)\n",
        "        class_random_preds = random_preds == c\n",
        "        class_random_f1 = binary_f1(class_random_preds, class_targets)\n",
        "\n",
        "        f1_by_class.append(class_f1)\n",
        "        random_f1_by_class.append(class_random_f1)\n",
        "\n",
        "    # We want to return tensors not lists\n",
        "    f1_by_class = torch.stack(f1_by_class)\n",
        "    random_f1_by_class = torch.stack(random_f1_by_class)\n",
        "\n",
        "    return f1_macro, random_f1_macro, f1_by_class, random_f1_by_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsM79Tloi-9m"
      },
      "outputs": [],
      "source": [
        "f1_macro, random_f1_macro, f1_by_class, random_f1_by_class = validate(val_dataloader, model, val_dataset.labels_sorted)\n",
        "print(\"F1 Macro for random predictions (1/39): \", random_f1_macro)\n",
        "print(\"F1 Macro for model predictions: \", f1_macro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR9y51Y__se6"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.bar(x=(0,1), height=(f1_macro.item(), random_f1_macro.item()), tick_label=(\"Model\", \"Random\"), color=(\"salmon\", \"skyblue\"))\n",
        "plt.title(\"F1 Macro\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w-R0lvBjSxi"
      },
      "source": [
        "Great! We were able to achieve better-than-chance results in terms of F1-Macro and Balanced Accuracy! Let's look at the results for individual classes and see which phonemes we were able to perform well on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeLpVGmWuLRT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_class_specific_scores(scores, random_scores, metric_name, labels, sort=True):\n",
        "\n",
        "    num_classes = len(labels)\n",
        "\n",
        "\n",
        "    # If sorting is requested, reorder the bars based on the criteria.\n",
        "    if sort:\n",
        "        order = torch.argsort(scores).flip(dims=[0])\n",
        "    else:\n",
        "        order = torch.arange(len(scores))\n",
        "\n",
        "    # Reorder the arrays along the class dimension (axis=1) and update the summary statistics\n",
        "    scores = scores[order]\n",
        "    random_scores = random_scores[order]\n",
        "    labels = [labels[i] for i in order]\n",
        "    # Positions of the groups on the x-axis\n",
        "    x = np.arange(num_classes)\n",
        "\n",
        "    # Width of each bar\n",
        "    width = 0.35\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(25, 12))\n",
        "\n",
        "    # Plot Random scores bars\n",
        "    bars1 = ax.bar(x - width/2, random_scores, width,\n",
        "                label='Random', capsize=5, color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Plot Actual score bars\n",
        "    bars2 = ax.bar(x + width/2, scores, width,\n",
        "                label='Model', capsize=5, color='salmon', edgecolor='black')\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel('Phonemes', fontsize=16)\n",
        "    ax.set_ylabel(metric_name, fontsize=16)\n",
        "    ax.set_title(metric_name + \" for each Phoneme\", fontsize=20)\n",
        "\n",
        "    # Set x-axis tick labels\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels, rotation=90, fontsize=16)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=14)\n",
        "\n",
        "    # Add grid for better readability\n",
        "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
        "\n",
        "    # Adjust layout to prevent clipping of tick-labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hEChyB3xGYU"
      },
      "outputs": [],
      "source": [
        "plot_class_specific_scores(scores=f1_by_class, random_scores=random_f1_by_class, metric_name=\"F1\", labels=val_dataset.labels_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk70pXaGhBFW"
      },
      "source": [
        "## That's it! ü•≥\n",
        "You've successfully trained a model that significantly outperforms random guessing in phoneme classification from MEG data - congrats! Thanks for taking the time to look at and/or participate in our competition. If you have any open questions, get in touch on [our Discord server](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord)!\n",
        "Once you're ready to get your score on the leaderboard, take a look at the [submission tutorial](https://neural-processing-lab.github.io/2025-libribrain-competition/links/submission-colab). You might also want to take another look at the [competition website](https://neural-processing-lab.github.io/2025-libribrain-competition)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7nmIZ85T-fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}